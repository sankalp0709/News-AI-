#!/usr/bin/env python3
"""
URL Issue Analysis and Solutions
"""

import requests
import json

def test_url_issues():
    """Test and analyze the URL issues reported by the user"""
    
    print("ğŸ” NEWS AI - URL ISSUE ANALYSIS")
    print("=" * 50)
    
    # Test URLs
    urls = [
        {
            "url": "https://www.youtube.com/watch?v=watch",
            "description": "Invalid YouTube URL (missing video ID)",
            "expected": "Should fail validation"
        },
        {
            "url": "https://x.com/narendramodi/status/1962435812588757337", 
            "description": "Twitter/X URL with bot protection",
            "expected": "Should show access restriction warning"
        },
        {
            "url": "https://www.youtube.com/watch?v=dQw4w9WgXcQ",
            "description": "Valid YouTube URL (Rick Roll)",
            "expected": "Should pass validation"
        }
    ]
    
    for i, test_case in enumerate(urls, 1):
        print(f"\n{i}. {test_case['description']}")
        print(f"   URL: {test_case['url']}")
        print(f"   Expected: {test_case['expected']}")
        
        # Test validation first
        try:
            validation_response = requests.post(
                "http://localhost:8000/api/validate-url",
                json={"url": test_case['url']}
            )
            
            if validation_response.status_code == 200:
                validation_data = validation_response.json()
                print(f"   âœ… Validation: {'PASS' if validation_data['success'] else 'FAIL'}")
                
                if not validation_data['success']:
                    print(f"   ğŸš« Issues: {', '.join(validation_data['data']['issues'])}")
                    print(f"   ğŸ’¡ Suggestions: {', '.join(validation_data['data']['suggestions'])}")
                    
                if validation_data['data']['issues']:
                    print(f"   âš ï¸  Warnings: {', '.join(validation_data['data']['issues'])}")
                    
            # Test scraping
            scrape_response = requests.post(
                "http://localhost:8000/api/scrape", 
                json={"url": test_case['url']}
            )
            
            if scrape_response.status_code == 200:
                scrape_data = scrape_response.json()
                print(f"   ğŸ“„ Scraping: {'SUCCESS' if scrape_data['success'] else 'FAILED'}")
                print(f"   ğŸ“ Title: {scrape_data['data']['title']}")
                print(f"   ğŸ“– Content Preview: {scrape_data['data']['content'][:80]}...")
                
        except Exception as e:
            print(f"   âŒ Error: {e}")
    
    print("\n" + "=" * 50)
    print("ğŸ”§ SOLUTIONS PROVIDED:")
    print("1. Enhanced URL validation to catch invalid YouTube URLs")
    print("2. Bot protection detection for social media platforms") 
    print("3. Clear error messages and suggestions for users")
    print("4. New /api/validate-url endpoint for pre-checking URLs")
    print("\nâœ¨ Your News AI system now handles these issues gracefully!")

if __name__ == "__main__":
    test_url_issues()